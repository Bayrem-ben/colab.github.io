{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BERT-arabert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ELjswHcFHfp3",
        "Pw80nRiJbHhN",
        "S6maM-vr7YaJ",
        "kNZl1lx_cA5Y"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bayrem-ben/colab.github.io/blob/main/Copy_of_BERT_arabert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGCJYkQj_Uu2"
      },
      "source": [
        "<h2 align=center> BERT-for-article-Classification</h2>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt3tjav7H5jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51f73eb-0ae4-4817-ecc2-1b1a0b64eaf5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpe6GhLuBJWB"
      },
      "source": [
        "### Check GPU Availability\n",
        "\n",
        "Check if your Colab notebook is configured to use Graphical Processing Units (GPUs). If zero GPUs are available, check if the Colab notebook is configured to use GPUs (Menu > Runtime > Change Runtime Type).\n",
        "\n",
        "![Hardware Accelerator Settings](https://drive.google.com/uc?id=1qrihuuMtvzXJHiRV8M7RngbxFYipXKQx)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V9c8vzSL3aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f49c88-549a-41e4-9d37-e32cae195402"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obch3rAuBVf0"
      },
      "source": [
        "### Install TensorFlow and TensorFlow Model Garden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUQEY3dFB0jX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae189d2a-fa61-40fe-dd83-09f89a53a02e"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.version.VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU3YLZ1TYKUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fa2173-3765-4f08-91a7-37391f81fead"
      },
      "source": [
        "!pip install -q tensorflow==2.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 320.4 MB 16 kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 138 kB/s \n",
            "\u001b[K     |████████████████████████████████| 459 kB 82.9 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFRTC-zwUy6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44168f0-3812-45b5-99d8-694a645d42c3"
      },
      "source": [
        "!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 2650, done.\u001b[K\n",
            "remote: Counting objects: 100% (2650/2650), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2311/2311), done.\u001b[K\n",
            "remote: Total 2650 (delta 506), reused 1388 (delta 306), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2650/2650), 34.02 MiB | 40.60 MiB/s, done.\n",
            "Resolving deltas: 100% (506/506), done.\n",
            "Note: checking out '400d68abbccda2f0f6609e3a924467718b144233'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H2G0571zLLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14529fb-062b-4d2f-8184-b4d88b7b318f"
      },
      "source": [
        "# install requirements to use tensorflow/models repository\n",
        "!pip install -Uqr models/official/requirements.txt\n",
        "# you may have to restart the runtime afterwards"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.6 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 202 kB 76.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 197 kB/s \n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 52.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 296 kB 71.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 12.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 41 kB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 55.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 32.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 73.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 92.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 82.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 25.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 54 kB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 79.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 23.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 44 kB 3.0 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.2 which is incompatible.\n",
            "tensorflow 2.3.0 requires scipy==1.4.1, but you have scipy 1.7.1 which is incompatible.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.28.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.1 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-storage 1.18.1 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-storage 1.18.1 requires google-resumable-media<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.3 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.1 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.1.1 which is incompatible.\n",
            "earthengine-api 0.1.284 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 2.26.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVjksk4yCXur"
      },
      "source": [
        "## Restart the Runtime\n",
        "\n",
        "**Note** \n",
        "After installing the required Python packages, you'll need to restart the Colab Runtime Engine (Menu > Runtime > Restart runtime...)\n",
        "\n",
        "![Restart of the Colab Runtime Engine](https://drive.google.com/uc?id=1xnjAy2sxIymKhydkqb0RKzgVK9rh3teH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmqEylyFYTdP"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import sys\n",
        "sys.path.append('models')\n",
        "from official.nlp.data import classifier_data_lib\n",
        "from official.nlp.bert import tokenization\n",
        "from official.nlp import optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuX1lB8pPJ-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4ab59f-66b6-4334-9c5d-a5883dc8543e"
      },
      "source": [
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Version:  2.6.0\n",
            "Eager mode:  True\n",
            "Hub version:  0.12.0\n",
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nI-9itVwCCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bedaf8c4-6c19-46b5-d1a0-86ba0794d525"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/press/article_ar_excel.xlsx\")\n",
        "#df = pd.read_csv('/content/drive/MyDrive/press/article_clean.csv')\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1282, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeHE98KiMvDd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e590c91b-b8d7-4bca-fcd7-385eb3af98aa"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Type</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>Category</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1277</th>\n",
              "      <td>الشركة الجهوية للنقل القوافل قفصة اعلان تمديد ...</td>\n",
              "      <td>طلب العروض</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>اقتصاد</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>استشارة عدد وع بس  اقتناء مواد ومستلزمات تنظيف...</td>\n",
              "      <td>الشركة والاستشارات</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ثقافة</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1279</th>\n",
              "      <td>إعلان بيع بالمزاد العلني</td>\n",
              "      <td>بيع بالمزاد العلني</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>تكنولوجيا</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>أن اختيار المياه المعدنية للشرب يساعد على إضاف...</td>\n",
              "      <td>صحة</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>صحة</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1281</th>\n",
              "      <td>تحسين صحة القلب من الشائع لدى الجميع أن اللحوم...</td>\n",
              "      <td>صحة</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Article  ... Unnamed: 6\n",
              "1277  الشركة الجهوية للنقل القوافل قفصة اعلان تمديد ...  ...        NaN\n",
              "1278  استشارة عدد وع بس  اقتناء مواد ومستلزمات تنظيف...  ...        NaN\n",
              "1279                           إعلان بيع بالمزاد العلني  ...        NaN\n",
              "1280  أن اختيار المياه المعدنية للشرب يساعد على إضاف...  ...        NaN\n",
              "1281  تحسين صحة القلب من الشائع لدى الجميع أن اللحوم...  ...        NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRTiyiLppxkV"
      },
      "source": [
        "df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Category', 'Unnamed: 6'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OPO8KdZpzVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d4dcdf-8313-40d5-e34f-306cea07a5fe"
      },
      "source": [
        "df['Article'].drop_duplicates(inplace=True)\n",
        "df['Article'].duplicated().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leRFRWJMocVa"
      },
      "source": [
        "df.plot(kind='hist', title='distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELjswHcFHfp3"
      },
      "source": [
        "## Task 4: Create tf.data.Datasets for Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0YWY81EH3tU"
      },
      "source": [
        "df_train, df_test = train_test_split(df, random_state=20, train_size=0.8, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fScULIGPwuWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e739887a-5dc6-4e7f-be9d-de52e7f836a0"
      },
      "source": [
        "df_train.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1025, 4), (257, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQYMGT5_qLPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a40a05e-e594-45c6-c373-7431256f094b"
      },
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  data = tf.data.Dataset.from_tensor_slices((df['Article_cleaned'].values, df['encodedLabel'].values))\n",
        "\n",
        "  for text, label in data.take(1):\n",
        "    print(text)\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'\\xd8\\xa3\\xd8\\xa8\\xd9\\x88\\xd8\\xb8\\xd8\\xa8\\xd9\\x8a \\xd8\\xa5\\xd9\\x8a\\xd9\\x85\\xd8\\xa7\\xd9\\x86 \\xd8\\xb3\\xd8\\xb1\\xd9\\x88\\xd8\\xb1 \\xd9\\x8a\\xd8\\xb4\\xd9\\x83\\xd9\\x84   \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xad\\xd9\\x88\\xd9\\x85 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x88\\xd8\\xb9\\xd8\\xa7\\xd8\\xa6\\xd9\\x84\\xd8\\xaa\\xd9\\x87   \\xd9\\x88\\xd8\\xa7\\xd8\\xad\\xd8\\xaf\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb4\\xd8\\xb1\\xd9\\x88\\xd8\\xb9\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x85\\xd9\\x8a\\xd8\\xb2\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x85\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a \\xd9\\x88\\xd9\\x8a\\xd8\\xb2\\xd9\\x8a\\xd8\\xaf \\xd8\\xa3\\xd8\\xb1\\xd8\\xb6 \\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd8\\xaa\\xd9\\x86\\xd8\\xa7\\xd8\\xb2\\xd9\\x84 \\xd8\\xa3\\xd8\\xa8\\xd9\\x86\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xad\\xd9\\x88\\xd9\\x85 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xac\\xd8\\xb2\\xd8\\xa1 \\xd8\\xad\\xd8\\xb5\\xd8\\xb5\\xd9\\x87\\xd9\\x85 \\xd8\\xa8\\xd9\\x85\\xd8\\xa8\\xd9\\x84\\xd8\\xba     \\xd8\\xaa\\xd8\\xae\\xd8\\xb5\\xd9\\x8a\\xd8\\xb5\\xd9\\x87 \\xd9\\x84\\xd9\\x85\\xd8\\xb5\\xd9\\x84\\xd8\\xad\\xd8\\xa9 \\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1 \\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a \\xd9\\x88\\xd9\\x81\\xd8\\xa7\\xd8\\xa1 \\xd9\\x84\\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd8\\xa7\\xd8\\xad\\xd9\\x84 \\xd9\\x88\\xd8\\xaa\\xd9\\x85\\xd8\\xb3\\xd9\\x83\\xd8\\xa7 \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd9\\x81\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xb5\\xd9\\x8a\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd8\\xa8\\xd8\\xaa\\xd8\\xba\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xac\\xd8\\xb1 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xab\\xd9\\x88\\xd8\\xa7\\xd8\\xa8 \\xd9\\x84\\xd9\\x8a\\xd8\\xb9\\xd9\\x85 \\xd8\\xb1\\xd9\\x8a\\xd8\\xb9   \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81   \\xd9\\x88\\xd9\\x86\\xd9\\x81\\xd8\\xb9\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd8\\xa6\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd8\\xad\\xd9\\x82\\xd8\\xa9\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xad\\xd9\\x88\\xd9\\x85 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x88\\xd8\\xb9\\xd8\\xa7\\xd8\\xa6\\xd9\\x84\\xd8\\xaa\\xd9\\x87 \\xd9\\x85\\xd8\\xaf\\xd9\\x89 \\xd8\\xa3\\xd8\\xb3\\xd8\\xb1\\xd8\\xa9 \\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x82\\xd8\\xaf\\xd9\\x85 \\xd9\\x82\\xd8\\xaf\\xd9\\x85 \\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa \\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x85\\xd9\\x84\\xd9\\x8a\\xd9\\x88\\xd9\\x86\\xd8\\xa7 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85\\xd8\\xa7 \\xd9\\x84\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd8\\xad\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x86\\xd8\\xb3\\xd8\\xa7\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x8a\\xd8\\xaa\\xd8\\xa7\\xd9\\x85 \\xd8\\xa5\\xd8\\xb6\\xd8\\xa7\\xd9\\x81\\xd8\\xa9 \\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd9\\x8a\\xd9\\x8a\\xd9\\x86 \\xd8\\xb5\\xd8\\xb1\\xd9\\x81\\xd9\\x87\\xd8\\xa7 \\xd8\\xaa\\xd9\\x86\\xd9\\x81\\xd9\\x8a\\xd8\\xb0 \\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd8\\xaa\\xd8\\xb1\\xd9\\x85\\xd9\\x8a\\xd9\\x85 \\xd9\\x88\\xd8\\xb5\\xd9\\x8a\\xd8\\xa7\\xd9\\x86\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xac\\xd8\\xaf \\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd9\\x84\\xd8\\xaf\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac \\xd9\\x85\\xd9\\x88\\xd8\\xb3\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa5\\xd9\\x8a\\xd9\\x81\\xd8\\xa7\\xd8\\xaf \\xd8\\xad\\xd8\\xac\\xd8\\xa7\\xd8\\xac \\xd9\\x88\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd8\\xa5\\xd9\\x81\\xd8\\xb7\\xd8\\xa7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd8\\xa7\\xd8\\xa6\\xd9\\x85 \\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd9\\x83\\xd8\\xb3\\xd9\\x88\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x8a\\xd8\\xaf \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xb6\\xd8\\xa7\\xd8\\xad\\xd9\\x8a \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x85\\xd8\\xb6\\xd8\\xa7\\xd9\\x86\\xd9\\x8a \\xd8\\xad\\xd8\\xb5\\xd9\\x8a\\xd9\\x84\\xd8\\xaa\\xd9\\x87\\xd8\\xa7 \\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd9\\x8a\\xd9\\x8a\\xd9\\x86 \\xd9\\x88\\xd8\\xa8\\xd9\\x84\\xd8\\xba\\xd8\\xaa \\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xae\\xd8\\xa7\\xd8\\xb1\\xd8\\xac \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x83\\xd8\\xa9 \\xd8\\xa8\\xd8\\xad\\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd8\\xba\\xd8\\xa7\\xd8\\xab\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd8\\xb9\\xd8\\xa8\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x84\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xad\\xd9\\x85\\xd8\\xb1 \\xd8\\xa3\\xd8\\xb3\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xb5\\xd9\\x8a\\xd8\\xa7\\xd9\\x86\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xac\\xd8\\xaf \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb2\\xd9\\x84 \\xd9\\x88\\xd8\\xaa\\xd9\\x85\\xd9\\x8a\\xd8\\xb2 \\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd9\\x87\\xd9\\x85\\xd8\\xa7\\xd8\\xaa \\xd9\\x82\\xd9\\x88\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xaa\\xd9\\x88\\xd8\\xa7\\xd8\\xb5\\xd9\\x84\\xd8\\xa9 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x85\\xd8\\xb9\\xd9\\x8a\\xd8\\xa7\\xd8\\xaa \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x87\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xb4\\xd9\\x85\\xd9\\x84 \\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa9 \\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa \\xd9\\x85\\xd8\\xaa\\xd9\\x86\\xd9\\x88\\xd8\\xb9\\xd8\\xa9 \\xd9\\x88\\xd8\\xa8\\xd8\\xad\\xd8\\xb3\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x86\\xd9\\x88\\xd9\\x8a \\xd9\\x84\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85 \\xd9\\x84\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xb7\\xd8\\xa7\\xd8\\xaa \\xd9\\x88\\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x84\\xd9\\x84\\xd9\\x81\\xd8\\xaa\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa5\\xd8\\xa8\\xd8\\xb1\\xd9\\x8a\\xd9\\x84uf\\xd9\\x86\\xd9\\x8a\\xd8\\xb3\\xd8\\xa7\\xd9\\x86 \\xd9\\x88\\xd8\\xad\\xd8\\xaa\\xd9\\x89 \\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3uf\\xd8\\xa2\\xd8\\xb0\\xd8\\xa7\\xd8\\xb1 \\xd9\\x82\\xd8\\xaf\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd9\\x85\\xd9\\x84\\xd9\\x8a\\xd9\\x88\\xd9\\x86\\xd8\\xa7 \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd8\\xaf\\xd8\\xb1\\xd9\\x87\\xd9\\x85\\xd8\\xa7 \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x81\\xd8\\xa7\\xd8\\xaf\\xd8\\xaa \\xd8\\xa2\\xd9\\x84\\xd8\\xa7\\xd9\\x81 \\xd8\\xac\\xd9\\x87\\xd8\\xa9 \\xd9\\x88\\xd8\\xb4\\xd8\\xae\\xd8\\xb5\\xd8\\xa7 \\xd9\\x88\\xd8\\xaa\\xd9\\x88\\xd8\\xb2\\xd8\\xb9\\xd8\\xaa \\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd9\\x88\\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac \\xd9\\x85\\xd9\\x88\\xd8\\xb3\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xaa\\xd9\\x88\\xd8\\xb2\\xd9\\x8a\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb2\\xd9\\x83\\xd8\\xa7\\xd8\\xa9 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xae\\xd8\\xb5\\xd8\\xb5 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd9\\x85\\xd8\\xac\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd9\\x8a\\xd9\\x8a\\xd9\\x86 \\xd8\\xa2\\xd9\\x84\\xd8\\xa7\\xd9\\x81 \\xd8\\xaf\\xd8\\xb1\\xd9\\x87\\xd9\\x85\\xd8\\xa7 \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x81\\xd8\\xa7\\xd8\\xaf \\xd8\\xa2\\xd9\\x84\\xd8\\xa7\\xd9\\x81 \\xd8\\xa3\\xd8\\xb3\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd9\\x81\\xd8\\xb1\\xd8\\xaf\\xd8\\xa7 \\xd8\\xae\\xd8\\xb5\\xd8\\xb5 \\xd9\\x85\\xd8\\xa8\\xd9\\x84\\xd8\\xba \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd8\\xaf\\xd8\\xb1\\xd9\\x87\\xd9\\x85\\xd8\\xa7 \\xd9\\x84\\xd9\\x84\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x88\\xd8\\xb3\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x81\\xd8\\xa7\\xd8\\xaf \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd9\\x81\\xd8\\xb1\\xd8\\xaf\\xd8\\xa7 \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd8\\xa3\\xd8\\xb3\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa8\\xd9\\x84\\xd8\\xba \\xd8\\xa5\\xd8\\xac\\xd9\\x85\\xd8\\xa7\\xd9\\x84\\xd9\\x8a \\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac \\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x85\\xd9\\x84\\xd9\\x8a\\xd9\\x88\\xd9\\x86\\xd8\\xa7 \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd8\\xaf\\xd8\\xb1\\xd9\\x87\\xd9\\x85\\xd8\\xa7 \\xd8\\xa8\\xd9\\x84\\xd8\\xba \\xd8\\xa5\\xd8\\xac\\xd9\\x85\\xd8\\xa7\\xd9\\x84\\xd9\\x8a \\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb1\\xd8\\xac\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa2\\xd9\\x84\\xd8\\xa7\\xd9\\x81 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x88\\xd8\\xb3\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xae\\xd8\\xb5\\xd8\\xb5 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd8\\xaf\\xd8\\xb1\\xd9\\x87\\xd9\\x85\\xd8\\xa7 \\xd8\\xae\\xd8\\xb5\\xd8\\xb5 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd9\\x85\\xd9\\x84\\xd9\\x8a\\xd9\\x88\\xd9\\x86\\xd8\\xa7 \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd9\\x84\\xd8\\xa8\\xd9\\x86\\xd8\\xaf \\xd8\\xaa\\xd9\\x88\\xd8\\xb2\\xd9\\x8a\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb2\\xd9\\x83\\xd8\\xa7\\xd8\\xa9 \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x81\\xd8\\xa7\\xd8\\xaf\\xd8\\xaa \\xd8\\xa3\\xd8\\xb3\\xd8\\xb1\\xd8\\xa9 \\xd9\\x81\\xd8\\xb1\\xd8\\xaf\\xd8\\xa7 \\xd9\\x88\\xd9\\x83\\xd8\\xb0\\xd9\\x84\\xd9\\x83 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xb9\\xd8\\xaf\\xd8\\xaf\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x86\\xd8\\xb3\\xd8\\xa7\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd8\\xad\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa8\\xd9\\x85\\xd9\\x84\\xd9\\x8a\\xd9\\x88\\xd9\\x86 \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd9\\x88\\xd9\\x82\\xd8\\xaf\\xd9\\x85 \\xd9\\x85\\xd8\\xa8\\xd9\\x84\\xd8\\xba \\xd8\\xa3\\xd9\\x84\\xd9\\x81\\xd8\\xa7 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85\\xd8\\xa7 \\xd9\\x84\\xd8\\xac\\xd8\\xa7\\xd8\\xa6\\xd8\\xb2\\xd8\\xa9 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x84\\xd9\\x84\\xd8\\xaa\\xd9\\x81\\xd9\\x88\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x84\\xd9\\x85\\xd9\\x8a \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x81\\xd8\\xa7\\xd8\\xaf \\xd8\\xb7\\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xa7 \\xd8\\xa3\\xd9\\x88\\xd8\\xa7\\xd8\\xa6\\xd9\\x84 \\xd9\\x85\\xd8\\xb9\\xd8\\xa7\\xd9\\x87\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x83\\xd9\\x86\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd9\\x82\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa3\\xd9\\x88\\xd8\\xa7\\xd8\\xa6\\xd9\\x84 \\xd8\\xac\\xd8\\xa7\\xd9\\x85\\xd8\\xb9\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xb5\\xd9\\x86 \\xd9\\x85\\xd8\\xa8\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xaf\\xd9\\x8a\\xd8\\xab \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa8\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x86\\xd8\\xb3\\xd8\\xa7\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa5\\xd9\\x86\\xd8\\xac\\xd8\\xa7\\xd8\\xb2\\xd9\\x87\\xd8\\xa7 \\xd8\\xa3\\xd9\\x83\\xd8\\xaf \\xd8\\xb9\\xd9\\x8a\\xd8\\xb3\\xd9\\x89 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x85\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85 \\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85   \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x84\\xd9\\x8a\\xd8\\xac   \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xad\\xd9\\x88\\xd9\\x85 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x87 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xb1\\xd8\\xad\\xd9\\x85\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87 \\xd8\\xb5\\xd8\\xa7\\xd8\\xad\\xd8\\xa8 \\xd9\\x85\\xd8\\xa8\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd9\\x81\\xd8\\xb1\\xd8\\xaf\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa3\\xd8\\xb1\\xd8\\xb3\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd8\\xab\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x88\\xd8\\xa7\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xba\\xd8\\xb0\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd8\\xba\\xd8\\xa7\\xd8\\xab\\xd9\\x8a\\xd8\\xa9 \\xd8\\xb7\\xd8\\xb1\\xd9\\x8a\\xd9\\x82 \\xd9\\x87\\xd9\\x8a\\xd8\\xa6\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x84\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xad\\xd9\\x85\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xaf\\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb7\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xaa\\xd8\\xa7\\xd8\\xac\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x84\\xd9\\x85 \\xd8\\xa3\\xd9\\x81\\xd8\\xba\\xd8\\xa7\\xd9\\x86\\xd8\\xb3\\xd8\\xaa\\xd8\\xa7\\xd9\\x86 \\xd9\\x88\\xd8\\xa8\\xd8\\xa7\\xd9\\x83\\xd8\\xb3\\xd8\\xaa\\xd8\\xa7\\xd9\\x86 \\xd9\\x88\\xd8\\xba\\xd9\\x8a\\xd8\\xb1\\xd9\\x87\\xd9\\x85\\xd8\\xa7 \\xd9\\x88\\xd8\\xb4\\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xaf\\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xac\\xd8\\xaf \\xd8\\xaa\\xd8\\xb1\\xd9\\x83\\xd9\\x8a\\xd8\\xa7 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xba\\xd8\\xb1\\xd8\\xa8 \\xd9\\x88\\xd8\\xb3\\xd9\\x84\\xd8\\xb7\\xd9\\x86\\xd8\\xa9 \\xd8\\xb9\\xd9\\x85\\xd8\\xa7\\xd9\\x86 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xb1\\xd8\\xaf\\xd9\\x86 \\xd8\\xba\\xd9\\x8a\\xd8\\xb1\\xd9\\x87\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x84\\xd8\\xaf\\xd8\\xa7\\xd9\\x86 \\xd8\\xa3\\xd8\\xa8\\xd9\\x88\\xd8\\xb8\\xd8\\xa8\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3 \\xd9\\x88\\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xaf\\xd9\\x88\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x84\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb1\\xd8\\xac \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85 \\xd9\\x85\\xd9\\x83\\xd8\\xaa\\xd8\\xa8\\xd8\\xa7 \\xd8\\xae\\xd8\\xa7\\xd8\\xb5\\xd8\\xa7 \\xd9\\x84\\xd8\\xaa\\xd8\\xb3\\xd8\\xaa\\xd9\\x85\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd9\\x8a\\xd8\\xb1\\xd8\\xa9 \\xd9\\x85\\xd9\\x86\\xd8\\xb8\\xd9\\x85 \\xd9\\x88\\xd9\\x85\\xd9\\x88\\xd8\\xab\\xd9\\x82 \\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xa7\\xd8\\xac\\xd8\\xb9\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xaa\\xd8\\xa7\\xd8\\xac\\xd9\\x8a\\xd9\\x86 \\xd8\\xb3\\xd9\\x88\\xd8\\xa7\\xd8\\xa1 \\xd9\\x85\\xd8\\xac\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x84\\xd8\\xa7\\xd8\\xac \\xd8\\xb3\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xa7\\xd8\\xac\\xd8\\xa9 \\xd9\\x84\\xd9\\x84\\xd8\\xa3\\xd8\\xb3\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x82\\xd9\\x8a\\xd8\\xb1\\xd8\\xa9 \\xd9\\x8a\\xd9\\x82\\xd9\\x88\\xd9\\x85 \\xd8\\xb4\\xd8\\xae\\xd8\\xb5\\xd9\\x8a\\xd8\\xa7 \\xd8\\xa8\\xd8\\xaa\\xd9\\x88\\xd8\\xb2\\xd9\\x8a\\xd8\\xb9 \\xd8\\xb5\\xd8\\xaf\\xd9\\x82\\xd8\\xa7\\xd8\\xaa \\xd9\\x88\\xd8\\xb2\\xd9\\x83\\xd8\\xa7\\xd8\\xa9 \\xd9\\x85\\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x88\\xd8\\xa8\\xd9\\x8a\\xd9\\x86 \\xd8\\xb9\\xd9\\x8a\\xd8\\xb3\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x85\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a \\xd9\\x85\\xd8\\xac\\xd8\\xa7\\xd9\\x84\\xd9\\x87 \\xd9\\x83\\xd8\\xa8\\xd9\\x8a\\xd8\\xb1 \\xd9\\x88\\xd8\\xa3\\xd9\\x81\\xd9\\x82\\xd9\\x87 \\xd9\\x88\\xd8\\xa7\\xd8\\xb3\\xd8\\xb9 \\xd9\\x88\\xd9\\x83\\xd9\\x84 \\xd8\\xa5\\xd9\\x85\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x8a\\xd8\\xa7\\xd8\\xaa\\xd9\\x87 \\xd9\\x8a\\xd8\\xb3\\xd8\\xaa\\xd8\\xb7\\xd9\\x8a\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd9\\x87\\xd9\\x85\\xd8\\xa9 \\xd8\\xaf\\xd8\\xa7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb7\\xd8\\xa7\\xd8\\xa1 \\xd9\\x88\\xd9\\x81\\xd9\\x8a\\xd9\\x85\\xd8\\xa7 \\xd9\\x8a\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x82 \\xd8\\xa8\\xd9\\x85\\xd8\\xaf\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x82\\xd8\\xa8\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x86\\xd8\\xb4\\xd8\\xb7\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa3\\xd8\\xa8\\xd9\\x86\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd8\\xb4\\xd8\\xaf\\xd8\\xaf \\xd8\\xb9\\xd9\\x8a\\xd8\\xb3\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd8\\xaf\\xd8\\xa7\\xd8\\xb1 \\xd9\\x84\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1 \\xd9\\x88\\xd8\\xa3\\xd8\\xa8\\xd9\\x86\\xd8\\xa7\\xd8\\xa4\\xd9\\x87\\xd8\\xa7 \\xd9\\x86\\xd9\\x87\\xd9\\x84\\xd9\\x88\\xd8\\xa7 \\xd8\\xad\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb7\\xd8\\xa7\\xd8\\xa1 \\xd9\\x85\\xd8\\xb9\\xd9\\x8a\\xd9\\x86 \\xd8\\xb2\\xd8\\xa7\\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1 \\xd9\\x88\\xd8\\xa3\\xd8\\xb5\\xd8\\xa8\\xd8\\xad \\xd9\\x81\\xd8\\xb7\\xd8\\xb1\\xd8\\xa9 \\xd9\\x84\\xd8\\xaf\\xd9\\x8a\\xd9\\x87\\xd9\\x85 \\xd9\\x86\\xd8\\xb1\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x8a\\xd8\\xac\\xd8\\xa7\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa8\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x88\\xd8\\xad \\xd8\\xaa\\xd8\\xaa\\xd8\\xad\\xd9\\x84\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x82\\xd9\\x8a\\xd8\\xa7\\xd8\\xaf\\xd8\\xa9 \\xd9\\x88\\xd8\\xb4\\xd8\\xb9\\xd8\\xa8\\xd8\\xa7 \\xd9\\x85\\xd8\\xb4\\xd9\\x8a\\xd8\\xb1\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb7\\xd8\\xa7\\xd8\\xa1 \\xd8\\xb4\\xd8\\xb9\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd9\\x85\\xd9\\x82\\xd8\\xaa\\xd8\\xb1\\xd9\\x86 \\xd8\\xa8\\xd9\\x85\\xd8\\xb5\\xd8\\xa7\\xd9\\x84\\xd8\\xad \\xd9\\x88\\xd8\\xa3\\xd9\\x86\\xd9\\x87 \\xd9\\x8a\\xd9\\x86\\xd8\\xb7\\xd9\\x84\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xb4\\xd9\\x87\\xd8\\xa7\\xd9\\x85\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x86\\xd8\\xae\\xd9\\x88\\xd8\\xa9 \\xd9\\x88\\xd9\\x85\\xd8\\xaf \\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x88\\xd9\\x86 \\xd9\\x88\\xd9\\x85\\xd8\\xa4\\xd8\\xa7\\xd8\\xb2\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb6\\xd8\\xb9\\xd9\\x81\\xd8\\xa7\\xd8\\xa1 \\xd9\\x88\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xaa\\xd8\\xa7\\xd8\\xac\\xd9\\x88\\xd9\\x82\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd9\\x85\\xd9\\x88\\xd8\\xac\\xd9\\x88\\xd8\\xaf \\xd9\\x85\\xd8\\xae\\xd8\\xaa\\xd9\\x84\\xd9\\x81 \\xd8\\xb3\\xd8\\xa7\\xd8\\xad\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x85\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x86\\xd8\\xb3\\xd8\\xa7\\xd9\\x86\\xd9\\x8a \\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd9\\x87\\xd9\\x85 \\xd8\\xa3\\xd8\\xb3\\xd8\\xa7\\xd8\\xb3\\xd9\\x8a \\xd9\\x85\\xd8\\xb3\\xd9\\x8a\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1 \\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd8\\xae\\xd8\\xa7\\xd8\\xb1\\xd8\\xac\\xd9\\x87\\xd8\\xa7 \\xd8\\xb4\\xd8\\xb1\\xd8\\xa7\\xd9\\x83\\xd8\\xa7\\xd8\\xaa \\xd9\\x85\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd9\\x8a\\xd8\\xa9 \\xd9\\x83\\xd8\\xab\\xd9\\x8a\\xd8\\xb1\\xd8\\xa9 \\xd9\\x88\\xd9\\x8a\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x83 \\xd8\\xa8\\xd9\\x82\\xd9\\x88\\xd8\\xa9 \\xd8\\xad\\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd8\\xba\\xd8\\xa7\\xd8\\xab\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd8\\xb9\\xd8\\xa8\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x84\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xad\\xd9\\x85\\xd8\\xb1 \\xd9\\x84\\xd8\\xaa\\xd9\\x82\\xd8\\xaf\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x88\\xd9\\x86 \\xd9\\x88\\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xaa\\xd8\\xb6\\xd8\\xb1\\xd8\\xb1\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x86\\xd8\\xa7\\xd8\\xb3 \\xd8\\xb3\\xd9\\x88\\xd8\\xa7\\xd8\\xa1 \\xd8\\xac\\xd8\\xb1\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xb1\\xd9\\x88\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd9\\x88\\xd8\\xa7\\xd8\\xb1\\xd8\\xab \\xd8\\xa7\\xd9\\x84\\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9\\xd9\\x88\\xd8\\xa3\\xd8\\xb6\\xd8\\xa7\\xd9\\x81 \\xd9\\x86\\xd8\\xb3\\xd8\\xac \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xb4\\xd8\\xb1\\xd8\\xa7\\xd9\\x83\\xd8\\xa7\\xd8\\xaa \\xd9\\x88\\xd8\\xa7\\xd8\\xb3\\xd8\\xb9\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x87\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd9\\x83\\xd9\\x88\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85\\xd9\\x84\\xd8\\xa9 \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xac\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x86\\xd8\\xb3\\xd8\\xa7\\xd9\\x86\\xd9\\x8a \\xd9\\x87\\xd9\\x8a\\xd8\\xa6\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x84\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xad\\xd9\\x85\\xd8\\xb1 \\xd9\\x88\\xd8\\xb5\\xd9\\x86\\xd8\\xaf\\xd9\\x88\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xb2\\xd9\\x83\\xd8\\xa7\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x8a\\xd8\\xa6\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85\\xd8\\xa9 \\xd9\\x84\\xd9\\x84\\xd8\\xb4\\xd8\\xa4\\xd9\\x88\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd8\\xb3\\xd9\\x84\\xd8\\xa7\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x88\\xd9\\x82\\xd8\\xa7\\xd9\\x81 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x82\\xd9\\x8a\\xd8\\xa7\\xd8\\xaf\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85\\xd8\\xa9 \\xd9\\x84\\xd8\\xb4\\xd8\\xb1\\xd8\\xb7\\xd8\\xa9 \\xd8\\xa3\\xd8\\xa8\\xd9\\x88\\xd8\\xb8\\xd8\\xa8\\xd9\\x8a \\xd9\\x88\\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xb4\\xd8\\xb1\\xd8\\xa7\\xd9\\x83\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x87\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xa8\\xd8\\xaa\\xd9\\x86\\xd9\\x81\\xd9\\x8a\\xd8\\xb0 \\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9\\xd9\\x87 \\xd9\\x88\\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac\\xd9\\x87 \\xd9\\x85\\xd9\\x86\\xd8\\xb3\\xd9\\x82 \\xd9\\x88\\xd9\\x85\\xd9\\x88\\xd8\\xab\\xd9\\x82 \\xd9\\x8a\\xd8\\xaa\\xd9\\x88\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xaa\\xd8\\xad\\xd9\\x82\\xd9\\x8a\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xb4\\xd8\\xb1\\xd8\\xa7\\xd9\\x83\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x87\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd8\\xa8\\xd8\\xad\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85\\xd9\\x84\\xd8\\xa9 \\xd8\\xa8\\xd9\\x85\\xd8\\xae\\xd8\\xaa\\xd9\\x84\\xd9\\x81 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xac\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xaa\\xd8\\xae\\xd8\\xaf\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd8\\xa6\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd9\\x87\\xd8\\xaf\\xd9\\x81\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd8\\xad\\xd9\\x82\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xb9 \\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd9\\x83\\xd8\\xa8\\xd8\\xa7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x86 \\xd9\\x88\\xd8\\xaa\\xd8\\xa3\\xd9\\x87\\xd9\\x8a\\xd9\\x84 \\xd8\\xb0\\xd9\\x88\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xad\\xd8\\xaa\\xd9\\x8a\\xd8\\xa7\\xd8\\xac\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb5\\xd8\\xa9 \\xd9\\x88\\xd9\\x83\\xd8\\xb0\\xd9\\x84\\xd9\\x83 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd8\\xad\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x85\\xd8\\xaa\\xd8\\xaf\\xd8\\xaa \\xd9\\x8a\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1 \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd9\\x84\\xd8\\xaa\\xd8\\xb4\\xd9\\x85\\xd9\\x84 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd9\\x85\\xd8\\xaf\\xd9\\x86\\xd9\\x87\\xd8\\xa7 \\xd8\\xad\\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xa5\\xd8\\xba\\xd8\\xa7\\xd8\\xab\\xd8\\xa9 \\xd9\\x85\\xd8\\xac\\xd9\\x85\\xd9\\x88\\xd8\\xb9\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xaa\\xd8\\xb3\\xd8\\xa7\\xd9\\x87\\xd9\\x85 \\xd8\\xb3\\xd9\\x86\\xd9\\x88\\xd9\\x8a\\xd8\\xa7 \\xd9\\x88\\xd8\\xa8\\xd8\\xb4\\xd9\\x83\\xd9\\x84 \\xd9\\x85\\xd9\\x86\\xd8\\xaa\\xd8\\xb8\\xd9\\x85 \\xd8\\xaa\\xd9\\x86\\xd8\\xb8\\xd9\\x8a\\xd9\\x85 \\xd8\\xad\\xd9\\x85\\xd9\\x84\\xd8\\xa9 \\xd9\\x84\\xd8\\xac\\xd9\\x85\\xd8\\xb9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xa8\\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd8\\xaa \\xd9\\x85\\xd9\\x88\\xd8\\xb8\\xd9\\x81\\xd9\\x8a\\xd9\\x87\\xd8\\xa7 \\xd8\\xb4\\xd9\\x87\\xd8\\xb1 \\xd8\\xb1\\xd9\\x85\\xd8\\xb6\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa8\\xd8\\xa7\\xd8\\xb1\\xd9\\x83 \\xd9\\x88\\xd8\\xaa\\xd9\\x82\\xd9\\x88\\xd9\\x85 \\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb4\\xd8\\xb1\\xd9\\x83\\xd8\\xa9 \\xd8\\xa8\\xd8\\xaa\\xd8\\xb3\\xd9\\x84\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xa8\\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd8\\xaa \\xd9\\x87\\xd9\\x8a\\xd8\\xa6\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x84\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xad\\xd9\\x85\\xd8\\xb1 \\xd9\\x84\\xd8\\xaa\\xd9\\x88\\xd8\\xb2\\xd9\\x8a\\xd8\\xb9\\xd9\\x87\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd9\\x81\\xd9\\x8a\\xd8\\xaf\\xd9\\x8a\\xd9\\x86 \\xd8\\xa8\\xd9\\x85\\xd8\\xb9\\xd8\\xb1\\xd9\\x81\\xd8\\xaa\\xd9\\x87 \\xd8\\xb3\\xd8\\xa7\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xa8\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd8\\xae\\xd8\\xa7\\xd8\\xb1\\xd8\\xac \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xb5\\xd9\\x8a\\xd8\\xa7\\xd9\\x86\\xd8\\xa9 \\xd9\\x88\\xd8\\xaa\\xd8\\xb1\\xd9\\x85\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xac\\xd8\\xaf \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb2\\xd9\\x84 \\xd9\\x88\\xd8\\xad\\xd9\\x81\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xa2\\xd8\\xa8\\xd8\\xa7\\xd8\\xb1 \\xd9\\x88\\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x86\\xd8\\xb3\\xd9\\x8a\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x84\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xad\\xd9\\x85\\xd8\\xb1 \\xd9\\x88\\xd8\\xad\\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xa5\\xd9\\x81\\xd8\\xb7\\xd8\\xa7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd8\\xa7\\xd8\\xa6\\xd9\\x85 \\xd8\\xaf\\xd9\\x88\\xd9\\x84 \\xd9\\x88\\xd8\\xa8\\xd9\\x84\\xd8\\xba\\xd8\\xaa \\xd9\\x82\\xd9\\x8a\\xd9\\x85\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xb1\\xd8\\xa7\\xd9\\x85\\xd8\\xac \\xd9\\x85\\xd9\\x84\\xd8\\xa7\\xd9\\x8a\\xd9\\x8a\\xd9\\x86 \\xd8\\xaf\\xd8\\xb1\\xd9\\x87\\xd9\\x85\\xd9\\x88\\xd9\\x8a\\xd8\\xae\\xd8\\xb5\\xd8\\xb5   \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85   \\xd9\\x85\\xd9\\x8a\\xd8\\xb2\\xd8\\xa7\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd8\\xb3\\xd9\\x86\\xd9\\x88\\xd9\\x8a\\xd8\\xa9 \\xd8\\xaa\\xd8\\xaa\\xd8\\xb1\\xd8\\xa7\\xd9\\x88\\xd8\\xad \\xd9\\x84\\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x86\\xd8\\xb3\\xd8\\xa7\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd8\\xad\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb5\\xd8\\xa9 \\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd9\\x88\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd8\\xae\\xd8\\xa7\\xd8\\xb1\\xd8\\xac\\xd9\\x87\\xd8\\xa7 \\xd8\\xa3\\xd8\\xb3\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa9 \\xd8\\xa8\\xd8\\xad\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xb9\\xd9\\x84\\xd8\\xa7\\xd8\\xac \\xd9\\x88\\xd8\\xaf\\xd9\\x8a\\xd9\\x88\\xd9\\x86 \\xd9\\x88\\xd8\\xa3\\xd8\\xad\\xd9\\x83\\xd8\\xa7\\xd9\\x85 \\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85 \\xd9\\x88\\xd8\\xaa\\xd8\\xa3\\xd9\\x87\\xd9\\x8a\\xd9\\x84 \\xd9\\x88\\xd8\\xac\\xd9\\x88\\xd8\\xa7\\xd8\\xa6\\xd8\\xb2 \\xd9\\x84\\xd9\\x84\\xd8\\xb7\\xd9\\x84\\xd8\\xa8\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xaa\\xd9\\x81\\xd9\\x88\\xd9\\x82\\xd9\\x8a\\xd9\\x86 \\xd8\\xa5\\xd8\\xb7\\xd8\\xa7\\xd8\\xb1 \\xd8\\xac\\xd8\\xa7\\xd8\\xa6\\xd8\\xb2\\xd8\\xa9 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x84\\xd9\\x84\\xd8\\xaa\\xd9\\x81\\xd9\\x88\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x84\\xd9\\x85\\xd9\\x8a \\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd9\\x84\\xd9\\x81\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd9\\x8a\\xd9\\x82\\xd8\\xaf\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd9\\x84\\xd8\\xb9\\xd8\\xaf\\xd8\\xaf \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xaf\\xd8\\xb1\\xd8\\xb3\\xd8\\xa9 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x88\\xd9\\x83\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd9\\x85 \\xd9\\x85\\xd8\\xa7\\xd9\\x84\\xd9\\x83 \\xd9\\x84\\xd9\\x84\\xd8\\xb4\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x82\\xd8\\xa7\\xd9\\x86\\xd9\\x88\\xd9\\x86 \\xd9\\x88\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd9\\x84\\xd9\\x87 \\xd9\\x85\\xd8\\xb3\\xd8\\xb9\\xd9\\x88\\xd8\\xaf \\xd9\\x84\\xd8\\xaa\\xd8\\xad\\xd9\\x81\\xd9\\x8a\\xd8\\xb8 \\xd8\\xa7\\xd9\\x84\\xd9\\x82\\xd8\\xb1\\xd8\\xa2\\xd9\\x86 \\xd9\\x88\\xd9\\x85\\xd8\\xac\\xd9\\x84\\xd8\\xb3 \\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xb4\\xd8\\xa4\\xd9\\x88\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x83\\xd8\\xa7\\xd8\\xaf\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x8a\\xd9\\x82\\xd8\\xaf\\xd9\\x85 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85\\xd8\\xa7 \\xd8\\xa8\\xd8\\xad\\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x8a \\xd8\\xb3\\xd9\\x86\\xd9\\x88\\xd9\\x8a\\xd8\\xa7 \\xd9\\x84\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd8\\xad\\xd9\\x8a\\xd8\\xa9 \\xd8\\xac\\xd9\\x85\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd9\\x84\\xd9\\x84\\xd8\\xab\\xd9\\x84\\xd8\\xa7\\xd8\\xb3\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa7 \\xd9\\x88\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x84\\xd9\\x8a\\xd8\\xac \\xd9\\x84\\xd9\\x84\\xd8\\xaa\\xd9\\x88\\xd8\\xad\\xd8\\xaf \\xd9\\x88\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2 \\xd8\\xa7\\xd9\\x84\\xd9\\x86\\xd9\\x88\\xd8\\xb1 \\xd9\\x84\\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd9\\x85 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd9\\x83\\xd9\\x85 \\xd9\\x88\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd9\\x82\\xd8\\xa8\\xd9\\x84 \\xd9\\x88\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2 \\xd8\\xb1\\xd8\\xa7\\xd8\\xb4\\xd8\\xaf \\xd9\\x84\\xd8\\xb9\\xd9\\x84\\xd8\\xa7\\xd8\\xac \\xd9\\x88\\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb7\\xd9\\x81\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd8\\xac\\xd9\\x85\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa \\xd9\\x84\\xd9\\x84\\xd8\\xa3\\xd9\\x85\\xd8\\xb1\\xd8\\xa7\\xd8\\xb6 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x8a\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa3\\xd8\\xb7\\xd8\\xa8\\xd8\\xa7\\xd8\\xa1 \\xd8\\xa8\\xd9\\x84\\xd8\\xa7 \\xd8\\xad\\xd8\\xaf\\xd9\\x88\\xd8\\xaf \\xd9\\x88\\xd9\\x85\\xd9\\x86\\xd8\\xb8\\xd9\\x85\\xd8\\xa7\\xd8\\xaa   \\xd8\\xb3\\xd8\\xa7\\xd9\\x8a\\xd8\\xaa \\xd8\\xb3\\xd9\\x8a\\xd9\\x81\\xd8\\xb1\\xd8\\xb2   \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd8\\xb9\\xd9\\x85 \\xd9\\x8a\\xd9\\x82\\xd8\\xaf\\xd9\\x85\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xad\\xd9\\x88\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x84\\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd9\\x81\\xd9\\x8a\\xd9\\x82\\xd8\\xaf\\xd8\\xb1 \\xd8\\xb3\\xd9\\x86\\xd9\\x88\\xd9\\x8a\\xd8\\xa7 \\xd9\\x88\\xd9\\x8a\\xd8\\xb4\\xd9\\x85\\xd9\\x84 \\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2 \\xd8\\xa7\\xd9\\x84\\xd8\\xa5\\xd8\\xad\\xd8\\xb3\\xd8\\xa7\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a \\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa9 \\xd8\\xad\\xd9\\x85\\xd9\\x8a\\xd8\\xaf \\xd8\\xb1\\xd8\\xa7\\xd8\\xb4\\xd8\\xaf \\xd9\\x84\\xd9\\x84\\xd8\\xa3\\xd8\\xb9\\xd9\\x85\\xd8\\xa7\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xb0\\xd9\\x83\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb1\\xd8\\xac\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd8\\xa7\\xd8\\xae\\xd9\\x84\\xd9\\x8a\\xd8\\xa9 \\xd9\\x8a\\xd9\\x82\\xd8\\xaf\\xd9\\x85\\xd9\\x87\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd8\\xb3\\xd9\\x86\\xd9\\x88\\xd9\\x8a\\xd8\\xa7 \\xd8\\xaa\\xd8\\xb4\\xd9\\x85\\xd9\\x84 \\xd9\\x85\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd8\\xb9 \\xd8\\xae\\xd9\\x8a\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85 \\xd8\\xaa\\xd8\\xb1\\xd9\\x85\\xd9\\x8a\\xd9\\x85 \\xd9\\x88\\xd8\\xb5\\xd9\\x8a\\xd8\\xa7\\xd9\\x86\\xd8\\xa9 \\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb2\\xd9\\x84 \\xd9\\x88\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xac\\xd8\\xaf \\xd9\\x88\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaa \\xd9\\x86\\xd9\\x82\\xd8\\xaf\\xd9\\x8a\\xd8\\xa9 \\xd9\\x84\\xd8\\xa3\\xd8\\xb3\\xd8\\xb1 \\xd9\\x85\\xd8\\xaa\\xd8\\xb9\\xd9\\x81\\xd9\\x81\\xd8\\xa9 \\xd8\\xaa\\xd9\\x85\\xd8\\xb1 \\xd8\\xa8\\xd8\\xb8\\xd8\\xb1\\xd9\\x88\\xd9\\x81 \\xd9\\x85\\xd9\\x81\\xd8\\xa7\\xd8\\xac\\xd8\\xa6\\xd8\\xa9 \\xd9\\x88\\xd8\\xad\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xb7\\xd8\\xa7\\xd8\\xb1\\xd8\\xa6\\xd8\\xa9 \\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xaa\\xd9\\x87\\xd8\\xa7 \\xd8\\xaa\\xd8\\xac\\xd8\\xa7\\xd9\\x88\\xd8\\xb2 \\xd9\\x85\\xd8\\xad\\xd9\\x86\\xd8\\xaa\\xd9\\x87\\xd8\\xa7 \\xd9\\x88\\xd8\\xb0\\xd9\\x84\\xd9\\x83 \\xd8\\xad\\xd8\\xa7\\xd8\\xac\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xb3\\xd8\\xb1\\xd8\\xa9 \\xd9\\x84\\xd8\\xa7\\xd9\\x81\\xd8\\xaa\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xa8\\xd9\\x86\\xd9\\x89 \\xd9\\x85\\xd8\\xb3\\xd8\\xac\\xd8\\xaf\\xd8\\xa7 \\xd8\\xaf\\xd9\\x88\\xd9\\x84 \\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9 \\xd9\\x88\\xd8\\xa5\\xd8\\xb3\\xd9\\x84\\xd8\\xa7\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x88\\xd8\\xaf\\xd8\\xa7\\xd9\\x86 \\xd9\\x88\\xd9\\x85\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd8\\xaa\\xd8\\xa7\\xd9\\x86\\xd9\\x8a\\xd8\\xa7 \\xd9\\x88\\xd8\\xb9\\xd9\\x85\\xd8\\xa7\\xd9\\x86 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x86\\xd8\\xaf \\xd9\\x88\\xd8\\xa8\\xd8\\xa7\\xd9\\x83\\xd8\\xb3\\xd8\\xaa\\xd8\\xa7\\xd9\\x86 \\xd9\\x83\\xd9\\x88\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xa4\\xd9\\x87\\xd9\\x84\\xd8\\xa9 \\xd9\\x86\\xd9\\x88\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd8\\xa8\\xd8\\xa3\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd9\\x88\\xd9\\x82\\xd9\\x81 \\xd8\\xae\\xd8\\xb5\\xd8\\xb5 \\xd9\\x85\\xd8\\xa8\\xd9\\x84\\xd8\\xba \\xd8\\xb3\\xd9\\x86\\xd9\\x88\\xd9\\x8a\\xd8\\xa7 \\xd9\\x84\\xd8\\xac\\xd8\\xa7\\xd8\\xa6\\xd8\\xb2\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd8\\xb1\\xd8\\xad\\xd9\\x88\\xd9\\x85 \\xd8\\xb9\\xd8\\xa8\\xd8\\xaf\\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd9\\x84\\xd9\\x8a\\xd9\\x84 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x84\\xd9\\x84\\xd8\\xaa\\xd9\\x81\\xd9\\x88\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x84\\xd9\\x85\\xd9\\x8a \\xd8\\xaa\\xd9\\x85\\xd9\\x86\\xd8\\xad \\xd9\\x84\\xd8\\xb7\\xd9\\x84\\xd8\\xa8\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x83\\xd9\\x86\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd9\\x82\\xd9\\x8a\\xd8\\xa9 \\xd9\\x8a\\xd8\\xae\\xd8\\xb5\\xd8\\xb5 \\xd8\\xaa\\xd9\\x85\\xd9\\x86\\xd8\\xad \\xd9\\x84\\xd9\\x84\\xd8\\xb9\\xd8\\xb4\\xd8\\xb1\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x88\\xd8\\xa7\\xd8\\xa6\\xd9\\x84 \\xd8\\xae\\xd8\\xb1\\xd9\\x8a\\xd8\\xac\\xd9\\x8a \\xd8\\xab\\xd8\\xa7\\xd9\\x86\\xd9\\x88\\xd9\\x8a\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x83\\xd9\\x86\\xd9\\x88\\xd9\\x84\\xd9\\x88\\xd8\\xac\\xd9\\x8a\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb7\\xd8\\xa8\\xd9\\x8a\\xd9\\x82\\xd9\\x8a\\xd8\\xa9 \\xd8\\xa8\\xd9\\x88\\xd8\\xa7\\xd9\\x82\\xd8\\xb9 \\xd9\\x84\\xd9\\x83\\xd9\\x84 \\xd8\\xb7\\xd8\\xa7\\xd9\\x84\\xd8\\xa8 \\xd8\\xb7\\xd8\\xa7\\xd9\\x84\\xd8\\xa8\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd8\\xa7\\xd8\\xa6\\xd8\\xb2\\xd8\\xa9 \\xd9\\x84\\xd9\\x84\\xd9\\x85\\xd8\\xaa\\xd9\\x81\\xd9\\x88\\xd9\\x82\\xd9\\x8a\\xd9\\x86 \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xb5\\xd9\\x81 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x86\\xd8\\xa7\\xd8\\xb7\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xb3\\xd8\\xaa\\xd9\\x88\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd9\\x85\\xd8\\xb1\\xd8\\xaa \\xd9\\x84\\xd9\\x86\\xd8\\xad\\xd9\\x88 \\xd8\\xaa\\xd8\\xb3\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xac\\xd8\\xa7\\xd8\\xa6\\xd8\\xb2\\xd8\\xa9 \\xd8\\xa8\\xd9\\x86\\xd8\\xa7\\xd8\\xa1 \\xd9\\x83\\xd9\\x88\\xd8\\xa7\\xd8\\xaf\\xd8\\xb1 \\xd8\\xa5\\xd9\\x85\\xd8\\xa7\\xd8\\xb1\\xd8\\xa7\\xd8\\xaa\\xd9\\x8a\\xd8\\xa9 \\xd9\\x85\\xd8\\xa4\\xd9\\x87\\xd9\\x84\\xd8\\xa9 \\xd9\\x85\\xd8\\xac\\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd9\\x87\\xd9\\x86\\xd8\\xaf\\xd8\\xb3\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x82\\xd9\\x86\\xd9\\x8a\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xaf\\xd9\\x8a\\xd8\\xab\\xd8\\xa9 \\xd8\\xaa\\xd8\\xb4\\xd8\\xac\\xd9\\x8a\\xd8\\xb9\\xd9\\x87\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xad\\xd8\\xa7\\xd9\\x82 \\xd8\\xa8\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x84\\xd9\\x8a\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x82\\xd9\\x86\\xd9\\x8a \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x87\\xd9\\x86\\xd9\\x8a \\xd8\\xa8\\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd9\\x88\\xd8\\xae\\xd9\\x84\\xd9\\x82 \\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd9\\x86\\xd8\\xa7\\xd9\\x81\\xd8\\xb3 \\xd8\\xa7\\xd9\\x84\\xd8\\xb4\\xd8\\xb1\\xd9\\x8a\\xd9\\x81 \\xd8\\xa8\\xd9\\x8a\\xd9\\x86\\xd9\\x87\\xd9\\x85 \\xd9\\x88\\xd9\\x82\\xd8\\xaf\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd9\\x81\\xd9\\x87\\xd9\\x8a\\xd9\\x85 \\xd9\\x85\\xd8\\xa4\\xd8\\xae\\xd8\\xb1\\xd8\\xa7 \\xd8\\xaf\\xd8\\xb9\\xd9\\x85\\xd8\\xa7 \\xd9\\x86\\xd9\\x82\\xd8\\xaf\\xd9\\x8a\\xd8\\xa7 \\xd9\\x84\\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xaa\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a\\xd8\\xaa\\xd9\\x8a\\xd9\\x86 \\xd8\\xaa\\xd8\\xb1\\xd8\\xb9\\xd9\\x8a\\xd8\\xa7\\xd9\\x86 \\xd8\\xb0\\xd9\\x88\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xad\\xd8\\xaa\\xd9\\x8a\\xd8\\xa7\\xd8\\xac\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb5\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x8a\\xd8\\xaa\\xd8\\xa7\\xd9\\x85   \\xd9\\x85\\xd8\\xa4\\xd8\\xb3\\xd8\\xb3\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd8\\xb4\\xd8\\xa7\\xd8\\xb1\\xd9\\x82\\xd8\\xa9 \\xd9\\x84\\xd9\\x84\\xd8\\xaa\\xd9\\x85\\xd9\\x83\\xd9\\x8a\\xd9\\x86 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xac\\xd8\\xaa\\xd9\\x85\\xd8\\xa7\\xd8\\xb9\\xd9\\x8a   \\xd9\\x84\\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd9\\x87\\xd9\\x85\\xd8\\xa9 \\xd8\\xb1\\xd8\\xb9\\xd8\\xa7\\xd9\\x8a\\xd8\\xa9 \\xd9\\x8a\\xd8\\xaa\\xd9\\x8a\\xd9\\x85   \\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2 \\xd8\\xa7\\xd9\\x84\\xd9\\x86\\xd8\\xac\\xd8\\xa7\\xd8\\xad \\xd9\\x84\\xd9\\x84\\xd8\\xaa\\xd8\\xa3\\xd9\\x87\\xd9\\x8a\\xd9\\x84   \\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd9\\x8a\\xd9\\x86 \\xd9\\x84\\xd9\\x85\\xd8\\xb3\\xd8\\xa7\\xd8\\xb9\\xd8\\xaf\\xd8\\xaa\\xd9\\x87 \\xd8\\xa5\\xd9\\x86\\xd8\\xac\\xd8\\xa7\\xd8\\xad \\xd8\\xa8\\xd8\\xb1\\xd9\\x86\\xd8\\xa7\\xd9\\x85\\xd8\\xac\\xd9\\x87 \\xd8\\xa7\\xd9\\x84\\xd9\\x85\\xd9\\x88\\xd8\\xac\\xd9\\x87 \\xd9\\x84\\xd8\\xaa\\xd8\\xa3\\xd9\\x87\\xd9\\x8a\\xd9\\x84 \\xd8\\xb0\\xd9\\x88\\xd9\\x8a \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xad\\xd8\\xaa\\xd9\\x8a\\xd8\\xa7\\xd8\\xac\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd8\\xa7\\xd8\\xb5\\xd8\\xa9 \\xd9\\x88\\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd9\\x8a\\xd8\\xaa\\xd8\\xa7\\xd9\\x85', shape=(), dtype=string)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2-ReN88Hvy_"
      },
      "source": [
        "## Task 5: Download a Pre-trained AraBERT Model from TensorFlow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjnb7B062Ha7"
      },
      "source": [
        "!git clone https://github.com/aub-mind/arabert.git\n",
        "!pip install pyarabic\n",
        "!pip install farasapy\n",
        "!pip install transformers\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMb5M86b4-BU"
      },
      "source": [
        "\"\"\"\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "# Label categories\n",
        "T_list = sorted(df['encodedLabel'].drop_duplicates())\n",
        "label_list = [T_list]\n",
        "max_seq_length = 512 # maximum length of (token) input sequences\n",
        "train_bash_size = 32\n",
        "\n",
        "# Get BERT layer and tokenizer:\n",
        "model_name=\"bert-base-arabertv2\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "\n",
        "arabert_prep.preprocess(df) \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Vc3dZal6Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e290d44-5f75-4290-a702-269210aa0a8a"
      },
      "source": [
        "#apply filter stop_word\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import argparse\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def stop_word():\n",
        "    Stop_w = list(set(stopwords.words('arabic')))\n",
        "    Tunisia_stop_words = '/content/drive/MyDrive/press/tunisian_stop_words.txt'\n",
        "    with open(Tunisia_stop_words, 'r+') as f:\n",
        "        lists = f.readlines() #readlines  .rstrip('\\n')\n",
        "    lines = []    \n",
        "    for line in lists:\n",
        "        stripped_line = line.rstrip('\\n')\n",
        "        lines.append(stripped_line)\n",
        "    stop_list = lines + Stop_w\n",
        "    stop_words = sorted(set(stop_list), key=lambda x:stop_list.index(x))\n",
        "    return stop_words\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                            َ    | # Fatha\n",
        "                            ً    | # Tanwin Fath\n",
        "                            ُ    | # Damma\n",
        "                            ٌ    | # Tanwin Damm\n",
        "                            ِ    | # Kasra\n",
        "                            ٍ    | # Tanwin Kasr\n",
        "                            ْ    | # Sukun\n",
        "                            ـ     # Tatwil/Kashida\n",
        "                        \"\"\", re.VERBOSE)\n",
        "    text = re.sub(arabic_diacritics, '', str(text))\n",
        "    return text\n",
        "\n",
        "def remove_emoji(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                          \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "def clean_text(text):\n",
        "    arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "    english_punctuations = string.punctuation\n",
        "    punctuations_list = arabic_punctuations + english_punctuations\n",
        "    text = \"\".join([word for word in text if word not in punctuations_list])\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_diacritics(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = stop_word()\n",
        "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "def prep_txt_arabert(text):\n",
        "    model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "    arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "    outputs = arabert_prep.preprocess(text)\n",
        "    txt_clean = clean_text(outputs)\n",
        "    #data['cleaned_text'] = data[txt_column].apply(clean_text)\n",
        "    #for row in data.itertuples():\n",
        "    text_clean = re.sub(r'[\\W\\s]', ' ', txt_clean) #row.cleaned_text))\n",
        "    return text_clean\n",
        "\n",
        "def prep_arabert(data, txt_column, Label_name):\n",
        "    model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "    arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "    List = []\n",
        "    cl_list = []\n",
        "    for txt in data[txt_column]:\n",
        "    #for txt in data.itertuples():\n",
        "        outputs = arabert_prep.preprocess(txt)\n",
        "        txt_clean = clean_text(outputs)\n",
        "        cl_list.append(txt_clean)\n",
        "    data['cleaned_text'] = cl_list\n",
        "    for row in data.itertuples():\n",
        "        List.append(re.sub(r'[\\W\\s]', ' ', row.cleaned_text)) #row.cleaned_text)) txt_clean\n",
        "    data['Article_cleaned'] = List\n",
        "    data = data.drop(columns=['cleaned_text'])\n",
        "    data = shuffle(data)\n",
        "    label_encoder = LabelEncoder()\n",
        "    data['encodedLabel'] = label_encoder.fit_transform(data[Label_name])\n",
        "    return data\n",
        "\n",
        "data = prep_arabert(df, 'Article', 'Type')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-10-16 21:25:21,928 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2021-10-16 21:25:27,107 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "إعلان بيع مزاد علني مر حضور ديواني\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "p5PHDY8-_Tp-",
        "outputId": "f19e1283-f815-4e44-b7e6-90f22f32959a"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Type</th>\n",
              "      <th>Article_cleaned</th>\n",
              "      <th>encodedLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1149</th>\n",
              "      <td>تعلم جمعية نادي الصحة الكاراتي بالمهدية أنها س...</td>\n",
              "      <td>إعلان</td>\n",
              "      <td>تعلم جمعي نادي صح كاراتي مهدي تعقد جلس خارق إث...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>وزارة الشؤون المحليةوالبيئة الديوان الوطني للت...</td>\n",
              "      <td>إعلان</td>\n",
              "      <td>وزار شؤون محليةوالبيئ ديوان وطني تطهير إعلان ب...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>['القاهرة:«الخليج» عاقبت محكمة جنايات شمال الق...</td>\n",
              "      <td>سياسة</td>\n",
              "      <td>قاهر   خليج   عاقب محكم جناي ات قاهر حازم صلاح...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>گراء أصل تجاري . وكالة حرة بمقتضى عقد بخط اليد...</td>\n",
              "      <td>أصل تجاري</td>\n",
              "      <td>أصل تجاري وكال حر مقتضى عقد خط يد تاريخ مسجل ق...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>الجمهورية التونسية وزارة الفلاحة والموارد الم...</td>\n",
              "      <td>إعلان</td>\n",
              "      <td>جمهوري تونسي وزار فلاح موارد مائي صيد بحري مند...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>['رأس الخيمة: «الخليج» نقل السفير الإثيوبي الس...</td>\n",
              "      <td>صحة</td>\n",
              "      <td>رأس خيم   خليج   نقل سفير إثيوبي سابق أستراليا...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>['أبوظبي «الخليج»: انخفض مؤشر سوق الإمارات الم...</td>\n",
              "      <td>اقتصاد</td>\n",
              "      <td>أبوظبي   خليج   انخفض مؤشر سوق إمار ات مالي صا...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>['عقدت اللجنة المنظمة للدورة الثالثة لمعرض تعا...</td>\n",
              "      <td>ثقافة</td>\n",
              "      <td>عقد لجن منظم دور معرض تعابير إماراتي رؤي تتحقق...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>['أظهرت نتائج بعض الشركات المساهمة والبنوك الو...</td>\n",
              "      <td>اقتصاد</td>\n",
              "      <td>أظهر نتائج شرك ات مساهم بنوك وطني تباين أرباح ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>907</th>\n",
              "      <td>['سيؤول أمين الجمال: استضافت السفارة الإماراتي...</td>\n",
              "      <td>صحة</td>\n",
              "      <td>سيؤول أمين جمال استضاف سفار إماراتي عاصم كوري ...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1282 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Article  ... encodedLabel\n",
              "1149  تعلم جمعية نادي الصحة الكاراتي بالمهدية أنها س...  ...            1\n",
              "197   وزارة الشؤون المحليةوالبيئة الديوان الوطني للت...  ...            1\n",
              "353   ['القاهرة:«الخليج» عاقبت محكمة جنايات شمال الق...  ...           10\n",
              "1234  گراء أصل تجاري . وكالة حرة بمقتضى عقد بخط اليد...  ...            0\n",
              "156    الجمهورية التونسية وزارة الفلاحة والموارد الم...  ...            1\n",
              "...                                                 ...  ...          ...\n",
              "873   ['رأس الخيمة: «الخليج» نقل السفير الإثيوبي الس...  ...           11\n",
              "511   ['أبوظبي «الخليج»: انخفض مؤشر سوق الإمارات الم...  ...            2\n",
              "568   ['عقدت اللجنة المنظمة للدورة الثالثة لمعرض تعا...  ...            7\n",
              "529   ['أظهرت نتائج بعض الشركات المساهمة والبنوك الو...  ...            2\n",
              "907   ['سيؤول أمين الجمال: استضافت السفارة الإماراتي...  ...           11\n",
              "\n",
              "[1282 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9oVmR3okYa3"
      },
      "source": [
        "data.to_csv(r'/content/drive/MyDrive/press/article_clean.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEUezMK-zkkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bab0583-1611-4672-9dfc-e3799731b6ff"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "tokenized_text = tokenizer.tokenize(\"إعلان بيع بالمزاد العلني للمرة الثالثة بحضور الديوانية\")\n",
        "clean_text = clean_text(tokenized_text)\n",
        "tokenized = tokenizer.tokenize(clean_text)\n",
        "input = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "print(tokenized_text)\n",
        "print(clean_text)\n",
        "print(tokenized)\n",
        "print(input)\n",
        "\n",
        "#model = hub.load('https://tfhub.dev/callmemehdi/AraBERT/1')\n",
        "#outputs = model(tf.convert_to_tensor([input], dtype=tf.int32))\n",
        "#print(outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['إعلان', 'بيع', 'بالم', '##زاد', 'العل', '##ني', 'ل', '##لم', '##رة', 'ال', '##ثالث', '##ة', 'بحضور', 'الدي', '##وانية']\n",
            "إعلانبيعبالم # # زادالعل # # نيل # # # # رةال # # # # ةبحضورالدي # # وانية\n",
            "['إعلان', '##بيع', '##بال', '##م', '#', '#', 'زاد', '##الع', '##ل', '#', '#', 'نيل', '#', '#', '#', '#', 'رة', '##ال', '#', '#', '#', '#', 'ة', '##بحضور', '##الدي', '#', '#', 'واني', '##ة']\n",
            "[1461, 2045, 3506, 199, 38, 38, 3139, 7042, 196, 38, 38, 3187, 38, 38, 38, 38, 32861, 292, 38, 38, 38, 38, 141, 31041, 50076, 38, 38, 21957, 251]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6YAXXHOgnLc"
      },
      "source": [
        "# convert to tf number better optimization and all\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw80nRiJbHhN"
      },
      "source": [
        "### **Install library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZgmxsWEbqzS"
      },
      "source": [
        "!git clone https://github.com/aub-mind/arabert.git\n",
        "!pip install pyarabic\n",
        "!pip install farasapy\n",
        "!pip install transformers\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0FvmRnxdmmf"
      },
      "source": [
        "!pip install PyArabic farasapy fast-bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcfTx519bUcY"
      },
      "source": [
        "!pkill \"java\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iet21PBVbOdo",
        "outputId": "0f454fa1-efde-4ad7-de69-acb7c05b3ab3"
      },
      "source": [
        "#Checking for GPU\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYhNL8gWafGQ"
      },
      "source": [
        "## **Youtube**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l21RGJ_ua7JY"
      },
      "source": [
        "import pandas as pd\n",
        "#from py4j.java_gateway import JavaGateway\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "#from arabert.preprocess_arabert import preprocess\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "farasa_segmenter = FarasaSegmenter(interactive=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_p_EiJjhLcP"
      },
      "source": [
        "### follow youtube\n",
        "df_AJGT = pd.read_csv('/content/drive/MyDrive/press/article_clean.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7fy9PEsmupF"
      },
      "source": [
        "DATA_COLUMN = 'Article_cleaned'\n",
        "LABEL_COLUMN = 'encodedLabel'\n",
        "\n",
        "df_AJGT = df_AJGT[['Article_cleaned', 'encodedLabel']]\n",
        "df_AJGT.columns = [DATA_COLUMN, LABEL_COLUMN]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK6TAKTpmuYN"
      },
      "source": [
        "#df_AJGT[DATA_COLUMN] = df_AJGT[DATA_COLUMN].apply(lambda x: ArabertPreprocessor(model_name=model_name).preprocess(x))\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import argparse\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "#tokenized = tokenizer.tokenize(df_AJGT[DATA_COLUMN])\n",
        "\n",
        "List = []\n",
        "cl_list = []\n",
        "model_name=\"bert-large-arabertv2\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
        "for txt in df_AJGT[DATA_COLUMN]:\n",
        "#for txt in df_AJGT.itertuples():\n",
        "    outputs = arabert_prep.preprocess(txt)\n",
        "    print(outputs)\n",
        "    List.append(outputs)\n",
        "\n",
        "df_AJGT[DATA_COLUMN] = List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-uHZzONxl75B",
        "outputId": "ef44acd8-97ec-4e34-f562-d426c532e182"
      },
      "source": [
        "df_AJGT.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article_cleaned</th>\n",
              "      <th>encodedLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>تعلم جمعي نادي صح كار +ات +ي مهدي تعقد جلس خار...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>و+ زار شؤون محليةوالبيئ ديوان وطني تطهير إعلان...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>قاهر خليج عاقب محكم جناي ات قاهر حازم صلاح إسم...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>أصل تجاري و+ كال حر مقتضى عقد خط يد تاريخ مسجل...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>جمهوري تونسي و+ زار ف+ لاح موارد مائي صيد بحري...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     Article_cleaned  encodedLabel\n",
              "0  تعلم جمعي نادي صح كار +ات +ي مهدي تعقد جلس خار...             1\n",
              "1  و+ زار شؤون محليةوالبيئ ديوان وطني تطهير إعلان...             1\n",
              "2  قاهر خليج عاقب محكم جناي ات قاهر حازم صلاح إسم...            10\n",
              "3  أصل تجاري و+ كال حر مقتضى عقد خط يد تاريخ مسجل...             0\n",
              "4  جمهوري تونسي و+ زار ف+ لاح موارد مائي صيد بحري...             1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLg3q-w-muVG"
      },
      "source": [
        "train_df = pd.DataFrame({\n",
        "    'id':range(len(df_AJGT)),\n",
        "    'label':df_AJGT[\"encodedLabel\"],\n",
        "    'alpha':['a']*df_AJGT.shape[0],\n",
        "    'text': df_AJGT[\"Article_cleaned\"].replace(r'\\n', ' ', regex=True)\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WllbCWcrBkC"
      },
      "source": [
        "!mkdir data\n",
        "train_df.to_csv(\"data/train.tsv\",index=False,columns=train_df.columns,sep='\\t',header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2MNDHD0o1Hp"
      },
      "source": [
        "# **Model Finetuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9il4gtlADcp"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import csv\n",
        "import sys\n",
        "from io import open\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm_notebook, trange\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers import BertPreTrainedModel\n",
        "from transformers import BertModel\n",
        "from transformers import WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer                                  \n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from arabert.preprocess import never_split_tokens\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5zVLupR0GwU"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "#https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1107"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9tEA_KUpFxK"
      },
      "source": [
        "csv.field_size_limit(2147483647)\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_predict_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class BinaryProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the binary data sets\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir , train_file_name):\n",
        "        \"\"\"See base class. file should be a tsv\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, train_file_name)), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir, dev_file_name):\n",
        "        \"\"\"See base class. file should be a tsv\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, dev_file_name)), \"dev\")\n",
        "\n",
        "    def get_predict_examples(self, data_dir, train_file_name):\n",
        "        \"\"\"See base class. file should be a tsv\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, train_file_name)), \"predict\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        if(set_type!=\"predict\"):\n",
        "            for (i, line) in enumerate(lines):\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = line[3]\n",
        "                label = line[1]\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        else:\n",
        "            for (i, line) in enumerate(lines):\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = line[3]\n",
        "                label = '0'\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def convert_example_to_feature(example_row, pad_token=0,\n",
        "sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "mask_padding_with_zero=True, sep_token_extra=False):\n",
        "    example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id, sep_token_extra = example_row\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "        tokens_b = tokenizer.tokenize(example.text_b)\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\". \" -4\" for RoBERTa.\n",
        "        special_tokens_count = 4 if sep_token_extra else 3\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - special_tokens_count)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "        special_tokens_count = 3 if sep_token_extra else 2\n",
        "        if len(tokens_a) > max_seq_length - special_tokens_count:\n",
        "            tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
        "\n",
        "    # The convention in BERT is:\n",
        "    # (a) For sequence pairs:\n",
        "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "    # (b) For single sequences:\n",
        "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "    #  type_ids:   0   0   0   0  0     0   0\n",
        "    #\n",
        "    # Where \"type_ids\" are used to indicate whether this is the first\n",
        "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "    # embedding vector (and position vector). This is not *strictly* necessary\n",
        "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "    # it easier for the model to learn the concept of sequences.\n",
        "    #\n",
        "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "    # the entire model is fine-tuned.\n",
        "    tokens = tokens_a + [sep_token]\n",
        "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "    if tokens_b:\n",
        "        tokens += tokens_b + [sep_token]\n",
        "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "    if cls_token_at_end:\n",
        "        tokens = tokens + [cls_token]\n",
        "        segment_ids = segment_ids + [cls_token_segment_id]\n",
        "    else:\n",
        "        tokens = [cls_token] + tokens\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding_length = max_seq_length - len(input_ids)\n",
        "    if pad_on_left:\n",
        "        input_ids = ([pad_token] * padding_length) + input_ids\n",
        "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "    else:\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    if output_mode == \"classification\":\n",
        "        label_id = label_map[example.label]\n",
        "    elif output_mode == \"regression\":\n",
        "        label_id = float(example.label)\n",
        "    else:\n",
        "        raise KeyError(output_mode)\n",
        "\n",
        "    return InputFeatures(input_ids=input_ids,\n",
        "                        input_mask=input_mask,\n",
        "                        segment_ids=segment_ids,\n",
        "                        label_id=label_id)\n",
        "  \n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                 tokenizer, output_mode,\n",
        "                                 cls_token_at_end=False, sep_token_extra=False, pad_on_left=False,\n",
        "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    examples = [(example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id, sep_token_extra) for example in examples]\n",
        "\n",
        "    process_count = cpu_count()\n",
        "\n",
        "    with Pool(process_count) as p:\n",
        "        features = list(tqdm(p.imap(convert_example_to_feature, examples, chunksize=500), total=len(examples)))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "processors = {\n",
        "    \"binary\": BinaryProcessor\n",
        "}\n",
        "\n",
        "output_modes = {\n",
        "    \"binary\": \"classification\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdvWdfqVpFt7"
      },
      "source": [
        "def load_and_cache_examples(task, tokenizer, mode=\"train\"):\n",
        "    processor = processors[task]()\n",
        "    output_mode = args['output_mode']\n",
        "    \n",
        "    cached_features_file = os.path.join(args['data_dir'], f\"cached_{mode}_{args['cache_dir']}_{args['max_seq_length']}_{task}\")\n",
        "    \n",
        "    if os.path.exists(cached_features_file) and not args['reprocess_input_data']:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "               \n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
        "        label_list = processor.get_labels()\n",
        "        if mode==\"train\":\n",
        "          examples=processor.get_train_examples(args['data_dir'],args['train_file_name']);\n",
        "        if mode==\"dev\":\n",
        "          examples=processor.get_dev_examples(args['data_dir'],args['dev_file_name']);\n",
        "        if mode==\"predict\":\n",
        "          examples=processor.get_predict_examples(args['data_dir'],args['pred_file_name'])\n",
        "        \n",
        "        if __name__ == \"__main__\":\n",
        "            features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n",
        "                cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n",
        "                cls_token=tokenizer.cls_token,\n",
        "                cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
        "                sep_token=tokenizer.sep_token,\n",
        "                sep_token_extra=bool(args['model_type'] in ['roberta']),           # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
        "                pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
        "                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n",
        "        \n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "        \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKimGs6lpFrg"
      },
      "source": [
        "def train(model, tokenizer):\n",
        "    tb_writer = SummaryWriter(log_dir=f\"{args['log_dir']}/{args['notes']}/seq_len_{args['max_seq_length']}_BS_{args['train_batch_size']}_lr_{args['learning_rate']}\")\n",
        "    \n",
        "    train_dataset = load_and_cache_examples(args['task_name'], tokenizer , mode=\"train\")\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
        "    \n",
        "    num_training_steps = len(train_dataloader) * args['num_train_epochs']\n",
        "    \n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    #model.named_parameters(): returns the name of the parameter as well as the parameter itself\n",
        "    #'bias', 'LayerNorm.weight' should have zero decay\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [param for name, param in model.named_parameters() if not any(nd in name for nd in no_decay)],\n",
        "         'weight_decay': args['weight_decay'],\n",
        "        },\n",
        "        {'params': [param for name, param in model.named_parameters() if any(nd in name for nd in no_decay)],\n",
        "         'weight_decay': 0.0,\n",
        "        }\n",
        "        ]\n",
        "    \n",
        "    #correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], correct_bias=True)\n",
        "    # PyTorch scheduler\n",
        "    # https://huggingface.co/transformers/main_classes/optimizer_schedules.html\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=num_training_steps)     \n",
        "\n",
        "    if args['fp16']:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
        "        \n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
        "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
        "    logger.info(\"  Total optimization steps = %d\", num_training_steps)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "\n",
        "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")    \n",
        "    for epoch in train_iterator:\n",
        "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            #model.train() tells your model that you are training the model.\n",
        "            #So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly\n",
        "            model.train()\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "            print(\"\\r%f\" % loss, end='')\n",
        "\n",
        "            if args['fp16']:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])                \n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
        "\n",
        "            tr_loss += loss.item() #trainning loss\n",
        "                \n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "            if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
        "                # Log metrics\n",
        "                tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args['logging_steps'], global_step)\n",
        "                logging_loss = tr_loss\n",
        "\n",
        "            if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
        "                # Save model checkpoint\n",
        "                output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
        "                if not os.path.exists(output_dir):\n",
        "                    os.makedirs(output_dir)\n",
        "                model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                model_to_save.save_pretrained(output_dir)\n",
        "                logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        if args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "            results = evaluate(model, tokenizer)\n",
        "            for key, value in results.items():\n",
        "                tb_writer.add_scalar('eval_{}'.format(key), value, epoch)\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7e1CITNpFpJ"
      },
      "source": [
        "from sklearn.metrics import (mean_squared_error, matthews_corrcoef, confusion_matrix,\n",
        "                             f1_score, precision_score , recall_score , accuracy_score)\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def get_mismatched(labels, preds):\n",
        "    mismatched = labels != preds\n",
        "    examples = processor.get_dev_examples(args['data_dir'],args['dev_file_name'])\n",
        "    wrong = [i for (i, v) in zip(examples, mismatched) if v]    \n",
        "    return wrong\n",
        "\n",
        "def get_eval_report(labels, preds):\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    return {\n",
        "        \"mcc\": mcc,\n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn\n",
        "    }, get_mismatched(labels, preds)\n",
        "\n",
        "def compute_metrics(task_name, preds, labels):\n",
        "\n",
        "    assert len(preds) == len(labels)\n",
        "    print(classification_report(labels,preds))\n",
        "    print(confusion_matrix(labels,preds))\n",
        "\n",
        "    f1_Positive = f1_score(labels,preds,pos_label=1,average='binary')\n",
        "    f1_Negative = f1_score(labels,preds,pos_label=0,average='binary')\n",
        "    macro_f1 = f1_score(labels,preds,average='macro')\n",
        "    macro_precision = precision_score(labels,preds,average='macro')\n",
        "    macro_recall = recall_score(labels,preds,average='macro')\n",
        "    acc = accuracy_score(labels,preds)\n",
        "    return {\n",
        "        'f1_pos': f1_Positive,\n",
        "        'f1_neg': f1_Negative,\n",
        "        'macro_f1' : macro_f1, \n",
        "        'macro_precision': macro_precision,\n",
        "        'macro_recall': macro_recall,\n",
        "        'accuracy': acc\n",
        "    }\n",
        "    #return get_eval_report(labels, preds)\n",
        "\n",
        "def evaluate(model, tokenizer, prefix=\"\"):\n",
        "    \n",
        "    eval_output_dir = args['output_dir']\n",
        "\n",
        "    results = {}\n",
        "    EVAL_TASK = args['task_name']\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, mode=\"dev\")\n",
        "    if not os.path.exists(eval_output_dir):\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    if args['output_mode'] == \"classification\":\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "    elif args['output_mode'] == \"regression\":\n",
        "        preds = np.squeeze(preds)\n",
        "\n",
        "    #result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
        "    results = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
        "    \n",
        "    return results#, wrong"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBVk7Z7AqqFk"
      },
      "source": [
        "def predict(model, tokenizer, prefix=\"\"):\n",
        "    pred_output_dir = args['output_dir']\n",
        "  \n",
        "    results={}\n",
        "    PRED_TASK = args['task_name']\n",
        "  \n",
        "    pred_dataset = load_and_cache_examples(PRED_TASK, tokenizer, mode='predict')\n",
        "    if not os.path.exists(pred_output_dir):\n",
        "      os.makedirs(pred_output_dir)\n",
        "    \n",
        "  \n",
        "    pred_sampler = SequentialSampler(pred_dataset)\n",
        "    pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=args['eval_batch_size'])\n",
        "  \n",
        "    logger.info(\"***** Running prediction {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(pred_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
        "  \n",
        "    preds = None\n",
        "    for batch in pred_dataloader:\n",
        "      with torch.no_grad():\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          inputs = {'input_ids': batch[0],'attention_mask': batch[1],'token_type_ids': batch[2],'labels': batch[3]}\n",
        "        \n",
        "          outputs = model(**inputs)\n",
        "          _, logits = outputs[:2]\n",
        "    if preds is None:\n",
        "        preds = logits.detach().cpu().numpy()\n",
        "    else:\n",
        "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "      \n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GelDLjtNqqD1"
      },
      "source": [
        "args = {\n",
        "    'data_dir': 'data/',\n",
        "    'train_file_name': 'train.tsv',\n",
        "    'dev_file_name': 'dev.tsv',\n",
        "    'pred_file_name': 'dev.tsv',\n",
        "    'model_type':  'bert',\n",
        "    'model_name': 'aubmindlab/bert-base-arabert',\n",
        "    'task_name': 'binary',\n",
        "    'output_dir': 'outputs_bert/',\n",
        "    'cache_dir': 'cache',\n",
        "    'do_train': True,\n",
        "    'do_eval': True,\n",
        "    'fp16': False,\n",
        "    'fp16_opt_level': 'O1',\n",
        "    'max_seq_length': 128,\n",
        "    'output_mode': 'classification',\n",
        "    'train_batch_size': 16,\n",
        "    'eval_batch_size': 32,\n",
        "    'num_train_epochs': 1,\n",
        "    'weight_decay': 0,\n",
        "    'learning_rate': 2e-5,\n",
        "    'adam_epsilon': 1e-8,\n",
        "    'warmup_steps': 0,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'log_dir':'/logs',\n",
        "    'logging_steps': 0,\n",
        "    'evaluate_during_training': True,\n",
        "    'save_steps': 90,\n",
        "    'eval_all_checkpoints': True,\n",
        "    'overwrite_output_dir': True,\n",
        "    'reprocess_input_data': False,\n",
        "    'notes': 'AJGT_arabert'\n",
        "}\n",
        "with open('args.json', 'w') as f:\n",
        "    json.dump(args, f)\n",
        "\n",
        "!mkdir ./{args['log_dir']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ythx3iMjqqAm"
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    #'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    #'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    #'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygvpfcWTqp9y"
      },
      "source": [
        "num_labels = df_AJGT['encodedLabel'].drop_duplicates()\n",
        "\n",
        "#https://github.com/huggingface/transformers/blob/master/src/transformers/configuration_bert.py#L52\n",
        "config = config_class.from_pretrained(args['model_name'], num_labels=len(num_labels), finetuning_task=args['task_name'])\n",
        "\n",
        "model = model_class.from_pretrained(args['model_name'], output_attentions=True)\n",
        "model.to(device)\n",
        "\n",
        "#https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py#L119\n",
        "tokenizer = tokenizer_class.from_pretrained(args['model_name'],\n",
        "    do_lower_case=False,\n",
        "    do_basic_tokenize=True,\n",
        "    never_split=never_split_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p1MkB52sdlJ"
      },
      "source": [
        "task = args['task_name']\n",
        "\n",
        "if task in processors.keys() and task in output_modes.keys():\n",
        "    processor = processors[task]()\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "else:\n",
        "    raise KeyError(f'{task} not found in processors or in output_modes. Please check utils.py.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbKY0UJksdiV"
      },
      "source": [
        "!rm -rf outputs_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFvL7oj8sdf0"
      },
      "source": [
        "#from transformers import AutoTokenizer\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabert')\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "global_step, tr_loss = train(model, tokenizer)\n",
        "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sageHkXisdc6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6maM-vr7YaJ"
      },
      "source": [
        "## Task 10: Fine-Tune BERT for Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5KWI3zjni5k"
      },
      "source": [
        "# Building the model\n",
        "def create_model():\n",
        "  input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids'),\n",
        "  input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask'),\n",
        "  input_type_ids =tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids'),\n",
        "\n",
        "  bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\", trainable=True)\n",
        "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "  drop = tf.keras.layers.Dropout(0.4)(pooled_output)\n",
        "  output = tf.keras.layers.Dense(1, activation='softmax', name=\"output\") (drop)\n",
        "  \n",
        "  model = tf.keras.Model(\n",
        "      input={'input_word_ids': input_word_ids, 'input_mask': input_mask, 'input_type_ids': input_type_ids},\n",
        "      output=output)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptCtiiONsBgo"
      },
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), \n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy(),\n",
        "              metrics=[tf.keras.metrics.categorical_accuracy()])\n",
        "model.summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJaFnkbMtPL"
      },
      "source": [
        "tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcREcgPUHr9O"
      },
      "source": [
        "# Train model\n",
        "epochs = 4\n",
        "history = model.fit(data, validation_data=0.8, epochs=epochs, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNZl1lx_cA5Y"
      },
      "source": [
        "## Task 11: Evaluate the BERT Text Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCjgrUYH_IsE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaWKcdVDUlWS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9z-c-AwYQdq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbj1IkojUlGC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}